# D2L--lesson10

## **丢弃法**

### **动机**

![img](https://i0.hdslb.com/bfs/note/cb8d0c1b03409c87f65d3fe9517841ff3e6416b4.png)

- 不管加入多少噪音，图片中的内容始终都是能够看清除的
- 正则项的作用就是使得权重不要过大，避免过拟合
- 在数据中加入噪音等价于一个正则
- 丢弃法是在层之间加入噪音，丢弃法其实是一个正则

### **无偏差的加入噪音**

![img](https://i0.hdslb.com/bfs/note/d5777f44c547dbf215cd1b57393834fe2aad03a3.png)

- 假设x是上一层到下一层的输出
- 对x加入噪音，但是不要改变它的期望
- 丢弃法：对于x’，给定一个概率p，在p的概率中将真是的原始的输入变成0，在其它的地方除以一个（1-p），即将该输入变大。在一部份地方以一定的概率p变成0，在另外一部分地方以一定的概率变大，所以它的期望是不变的



### **使用丢弃法**

![img](https://i0.hdslb.com/bfs/note/d37f21d94fb1f467f059a75c1cbe502b922fbf8d.png)

- h是第一个隐藏层的输出
- dropout表示对h使用丢弃法

### **推理中的丢弃法**

![img](https://i0.hdslb.com/bfs/note/9a4b32207d9d2bdda21e340303fd7cc126a877ae.png)

- 在预测的过程中因为不训练，所以不使用dropout
- 正则项只在训练中使用，它只会对权重产生影响，在预测的时候，权重不需要发生变化的情况下，不需要正则，所以在推理中不需要正则，确保在推理过程中能够有一个确定性的输出

**总结**



![img](https://i0.hdslb.com/bfs/note/cac24e35e2db1785269ec44b90ca601bf85cb95c.png)

- 常用在多层感知机的隐藏层输出上，就是对应的全连接层的额隐藏层输出上，很少会用在CNN这类的模型上面